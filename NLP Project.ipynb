{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions:\n",
    "\n",
    "- For POS tagging, should I used filtered list or tokens? (Different results)\n",
    "        #self.pos_tags = self.parts_of_speech_tagging(self.filtered_text)\n",
    "- Do I keep ANP? The text file is pretty bad. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Things done so far:\n",
    "\n",
    "- Converted pdf and doc files to txt files\n",
    "- Create a manifesto class (object oriented programming)\n",
    "- Read each manifesto text file\n",
    "- Created word tokens of each text file\n",
    "- Created sentence tokens of each text file\n",
    "- Find word frequency of words in each text file\n",
    "- Do parts of speech tagging for each text file\n",
    "- Create a 'stemmed list' for the tokens in each text file\n",
    "- Pre-processing to remove stop words and convert to lower case to avoid repetition\n",
    "- Find any top X number of most frequent words in each text file\n",
    "- Create a word cloud of any given text, for any optional number of words\n",
    "- Summarize text from any text file\n",
    "- Creates ngrams of any length, and presents any X most common ngrams\n",
    "- Calculates document similarity between any two manifestos\n",
    "- Calculates sentence Sentiment (polarity and subjectivity) and plots graphs for each\n",
    "\n",
    "\n",
    "### Packages Used:\n",
    "- NLTK\n",
    "- Gensim\n",
    "- Word Cloud\n",
    "- Matplotlib\n",
    "- Collections\n",
    "- Spacy\n",
    "- TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import RSLPStemmer\n",
    "from nltk import pos_tag\n",
    "from nltk.util import ngrams   \n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "import matplotlib.pyplot as plt\n",
    "from gensim.summarization.summarizer import summarize\n",
    "from collections import Counter\n",
    "import spacy\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "\n",
    "class Manifesto(object):\n",
    "    '''\n",
    "    Creates a Manifesto object.\n",
    "    Can be used to assess different aspects of a text file\n",
    "        including tokens, most common words, parts of speech.\n",
    "    Can be used to create word clouds.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __init__(self, file_path, name):\n",
    "        '''\n",
    "        Initializes the object variables.\n",
    "        '''\n",
    "        self.name = name\n",
    "        self.text = self.reading_file (file_path)\n",
    "        self.tokens = self.tokenize (self.text)\n",
    "        self.sentence_tokens = token_sentences(self.text)\n",
    "        self.filtered_text = self.preprocessing (self.tokens)\n",
    "        self.stemmed_list = self.stemmer(self.filtered_text)\n",
    "        self.pos_tags = self.parts_of_speech_tagging(self.tokens)\n",
    "        self.word_frequency = self.finding_word_frequency(self.filtered_text)\n",
    "        \n",
    "    \n",
    "    def reading_file (self, file_path):\n",
    "        '''\n",
    "        Given a file path, checks if file exists there, reads it, closes it,\n",
    "            and returns the text as a string.\n",
    "\n",
    "        Input:\n",
    "            file_path: string\n",
    "\n",
    "        Return:\n",
    "            text: string\n",
    "        '''\n",
    "        assert os.path.exists(file_path), \"File not found at: \"+str(file_path)\n",
    "        f = open(file_path,'r')    \n",
    "        text = f.read()\n",
    "        f.close()\n",
    "        return text\n",
    "        \n",
    "        \n",
    "    def tokenize (self, text):\n",
    "        '''\n",
    "        Given some text, will return tokens of that text\n",
    "        \n",
    "        Input: \n",
    "            text: string\n",
    "        Output:\n",
    "            token: list of string\n",
    "        '''\n",
    "        tokens = nltk.word_tokenize(text)\n",
    "        return tokens\n",
    "    \n",
    "    \n",
    "    def token_sentences(self, text):\n",
    "        '''\n",
    "        Given some text, will return sentence tokens of that text\n",
    "        \n",
    "        Input:\n",
    "            text: string\n",
    "        \n",
    "        Output:\n",
    "            list of sentence tokens (string)\n",
    "        '''\n",
    "        sent_text = nltk.sent_tokenize(text)\n",
    "        return sent_text\n",
    "\n",
    "        \n",
    "    def finding_word_frequency (self, filtered_text):\n",
    "        '''\n",
    "        Given a list, returns the word frequency\n",
    "        \n",
    "        Input:\n",
    "            filtered_text: list of strings\n",
    "        Output:\n",
    "            word frequency: nltk.probability.FreqDist\n",
    "        '''\n",
    "        word_frequency = nltk.FreqDist(filtered_text)\n",
    "        return word_frequency\n",
    "        \n",
    "        \n",
    "    def parts_of_speech_tagging(self, tokens):\n",
    "        '''\n",
    "        Given tokens, assigns parts of speech to each token\n",
    "        \n",
    "        '''\n",
    "        tagged = nltk.pos_tag(tokens)\n",
    "        return tagged      \n",
    "        \n",
    "        \n",
    "    def stemmer (self, filtered_text):\n",
    "        '''\n",
    "        Does stemming/lemmatization of a given text\n",
    "        Input:\n",
    "            filtered_text: list of string\n",
    "            \n",
    "        Output:\n",
    "            a set of stemmed words\n",
    "        '''\n",
    "        st = RSLPStemmer()\n",
    "        stemmed_list = set(st.stem(token) for token in filtered_text)\n",
    "        return stemmed_list\n",
    "        \n",
    "        \n",
    "        \n",
    "    def preprocessing (self, text):\n",
    "        '''\n",
    "        Removes stop words and converts to lower case.\n",
    "        \n",
    "        Input:\n",
    "            text: string\n",
    "            \n",
    "        Output:\n",
    "            filtered_text: list of string\n",
    "        '''\n",
    "        stop_words = set(stopwords.words('english'))\n",
    "        words=[word.lower() for word in text if word.isalpha()]\n",
    "        filtered_text = [w for w in words if not w in stop_words]\n",
    "        return filtered_text\n",
    "\n",
    "    \n",
    "        \n",
    "    def find_most_frequent_words(self, number):\n",
    "        '''\n",
    "        For a given manifesto object, returns the most common X number of words used\n",
    "            along with the count\n",
    "            \n",
    "        Input:\n",
    "            number: integer\n",
    "            \n",
    "        Output:\n",
    "            mostcommon: list\n",
    "        '''\n",
    "        wordfreqdist = nltk.FreqDist(self.filtered_text)\n",
    "        mostcommon = wordfreqdist.most_common(number)\n",
    "        return mostcommon\n",
    "        \n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        \n",
    "        return (self.name + ' Manifesto ')\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "def create_wordcloud(text, title = None, maximum_words = 100):\n",
    "    '''\n",
    "    Creates a word cloud based on the text of the file.\n",
    "    Removes stop words (which consists of conventional stop words  \n",
    "            and words from my own list)\n",
    "            \n",
    "    Input:\n",
    "        text (string)\n",
    "        title (optional, string)\n",
    "        maximum words (integer, default = 100)\n",
    "        \n",
    "\n",
    "    Special thanks to the community at stackoverflow\n",
    "    (https://stackoverflow.com/questions/16645799/how-to-create-a-word-cloud-from-a-corpus-in-python)\n",
    "    for this one!\n",
    "    '''\n",
    "    stop_words = list(STOPWORDS)\n",
    "    personal_list = ['pakistan', 'people', 'party', 'manifesto', 'government', 'per', \n",
    "                    'cent', 'will', 'Parliamentarians', 'ANP', 'MQM', 'iii', 'i', 'ii', 'iv', 'v', 'vi','vii', 'PML', 'PTI', 'ensure', 'right', 'provide' ]\n",
    "    stop_words_2 = set(stop_words + personal_list)\n",
    "\n",
    "\n",
    "    wordcloud = WordCloud(\n",
    "        background_color='white',\n",
    "        stopwords=stop_words_2,\n",
    "        max_words=maximum_words,\n",
    "        scale=3,\n",
    "        max_font_size=40\n",
    "    ).generate(str(text))\n",
    "\n",
    "    fig = plt.figure(1, figsize=(20, 20), dpi = 400)\n",
    "    plt.axis('off')\n",
    "    if title: \n",
    "        fig.suptitle(title, fontsize=30)\n",
    "        fig.subplots_adjust(top=2.3)\n",
    "\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "def summarize_text (text):\n",
    "    '''\n",
    "    Given some text, will summarize it and return the summary\n",
    "        \n",
    "    Input:\n",
    "        text (string)\n",
    "    Output:\n",
    "        summary (string)\n",
    "    '''\n",
    "    return summarize(text)\n",
    "\n",
    "\n",
    "def getting_ngrams(text, gram_type, length_of_list):\n",
    "    '''\n",
    "    Given a text, given the type of gram (example, bigram, trigram, fourgram etc), and given length of list,\n",
    "        returns a list of the most common X ngrams in that text. \n",
    "        \n",
    "    Input:\n",
    "        text (string)\n",
    "        gram_type (integer): indicates the number of grams\n",
    "        length_of_list (integer): indicates the number of most common ngrams to show\n",
    "    \n",
    "    Output:\n",
    "        ngram (list of string)\n",
    "        \n",
    "    Thanks to the Stackoverflow community at \n",
    "    https://stackoverflow.com/questions/32441605/generating-ngrams-unigrams-bigrams-etc-from-a-large-corpus-of-txt-files-and-t\n",
    "    for help with this!\n",
    "    '''\n",
    "    gram = ngrams(text, gram_type)\n",
    "    ngram = Counter(gram).most_common(length_of_list)\n",
    "    return ngram\n",
    "\n",
    "\n",
    "\n",
    "def manifesto_similarities():\n",
    "    '''\n",
    "    Prints similarity score between the manifestos of different parties using manifesto text.\n",
    "    Uses package 'spacy' to calculate similarity\n",
    "    \n",
    "    '''\n",
    "    nlp = spacy.load('en')\n",
    "    list_of_parties = [ppp, pmln, mqm, pti, anp, ji]\n",
    "    tokens = [nlp(ppp.text), nlp(pmln.text), nlp(mqm.text), nlp(pti.text), nlp(anp.text), nlp(ji.text)]\n",
    "\n",
    "    counter_1 = 0\n",
    "    print ('{} {:>30} {:>40}'.format('PARTY 1', 'PARTY 2','SIMILARITY SCORES'))\n",
    "    print ()\n",
    "    for token1 in tokens:    \n",
    "        counter_2 = 0\n",
    "        for token2 in tokens:\n",
    "            print ('{:30} {:30} {:.5}'.format(list_of_parties[counter_1].name, list_of_parties[counter_2].name,token1.similarity(token2) ))\n",
    "            counter_2 = counter_2 + 1\n",
    "        counter_1 = counter_1 + 1\n",
    "        print ()\n",
    "        \n",
    "        \n",
    "def sentiment_analysis (sentence_tokens, name):\n",
    "    '''\n",
    "    Given sentence tokens, calculates polarity and subjectivity of each sentence in the document,\n",
    "        and plots graphs of polarity and subjectivity\n",
    "        \n",
    "    Input:\n",
    "        sentence_tokens: list of strings\n",
    "        name: the name of each party (string)\n",
    "    \n",
    "    '''\n",
    "    polarity = []\n",
    "    subjectivity = []\n",
    "    for sentence in sentence_tokens:\n",
    "        s = TextBlob(sentence)\n",
    "        polarity.append(s.sentiment[0])\n",
    "        subjectivity.append(s.sentiment[1])\n",
    "        \n",
    "    plt.plot(polarity)\n",
    "    plt.xlabel('Sentences across manifesto')\n",
    "    plt.ylabel('Polarity')\n",
    "    plt.title(' Manifesto of '+ name)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    plt.plot(subjectivity, 'C7')\n",
    "    plt.xlabel('Sentences across manifesto')\n",
    "    plt.ylabel('subjectivity')\n",
    "    plt.title(' Manifesto of '+ name)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# CREATING MANIFESTO OBJECTS OF THE POLITICAL PARTIES\n",
    "\n",
    "ppp = Manifesto ('/Users/kazi/Desktop/Manifesto Text Files/PPP_2013.txt', 'Pakistan Peoples Party')\n",
    "pmln = Manifesto ('/Users/kazi/Desktop/Manifesto Text Files/PMLN_2013.txt', 'Pakistan Muslim League N')\n",
    "mqm = Manifesto ('/Users/kazi/Desktop/Manifesto Text Files/MQM_2013.txt', 'Mutahhida Qaumi Movement')\n",
    "pti = Manifesto ('/Users/kazi/Desktop/Manifesto Text Files/PTI_2013.txt', 'Pakistan Tehreek-e-Insaaf')\n",
    "anp = Manifesto ('/Users/kazi/Desktop/Manifesto Text Files/ANP_2013.txt', 'Awami National Party')\n",
    "ji = Manifesto ('/Users/kazi/Desktop/Manifesto Text Files/JI_2013.txt', 'Jamat-ul-Islami')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "ppp = Manifesto ('/Users/kazi/Desktop/Manifesto Text Files/PPP_2013.txt', 'Pakistan Peoples Party')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
